<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection Attendance</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
            flex-direction: column;
        }
        video {
            border: 2px solid black;
        }
        .student-name {
            font-size: 16px;
            font-weight: bold;
            color: green;
            position: absolute;
            top: 10px;
            left: 10px;
            z-index: 10;
        }
    </style>
</head>
<body>

    <h1>Face Recognition Attendance</h1>
    <video id="video" width="640" height="480" autoplay playsinline></video>
    <canvas id="overlay" width="640" height="480"></canvas>
    <div class="student-name" id="studentName"></div>

    <script>
        const videoElement = document.getElementById('video');
        const overlay = document.getElementById('overlay');
        const studentNameElement = document.getElementById('studentName');

        // Load face-api.js models
        Promise.all([
            faceapi.nets.ssdMobilenetv1.loadFromUri('models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('models')
        ]).then(startVideo);

        // Access the back camera
        async function startVideo() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment' }
                });
                videoElement.srcObject = stream;
            } catch (error) {
                console.error("Camera error: ", error);
            }
        }

        // Start detecting faces once the video starts playing
        videoElement.addEventListener('play', () => {
            const displaySize = { width: videoElement.width, height: videoElement.height };
            faceapi.matchDimensions(overlay, displaySize);

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(videoElement)
                    .withFaceLandmarks()
                    .withFaceDescriptors();

                // Clear overlay canvas
                const ctx = overlay.getContext('2d');
                ctx.clearRect(0, 0, overlay.width, overlay.height);

                // Resize and draw the detections
                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                faceapi.draw.drawDetections(overlay, resizedDetections);

                // Identify detected faces
                for (let i = 0; i < detections.length; i++) {
                    const bestMatch = await recognizeFace(detections[i].descriptor);
                    if (bestMatch) {
                        const box = resizedDetections[i].detection.box;
                        studentNameElement.style.top = `${box.top}px`;
                        studentNameElement.style.left = `${box.left}px`;
                        studentNameElement.textContent = bestMatch;
                    }
                }
            }, 100);
        });

        // Match face descriptors with known students
        async function recognizeFace(queryDescriptor) {
            const knownDescriptors = await loadKnownFaces();
            let bestMatch = null;
            let minDistance = 0.6; // Threshold

            for (const student of knownDescriptors) {
                const distance = faceapi.euclideanDistance(queryDescriptor, student.descriptor);
                if (distance < minDistance) {
                    minDistance = distance;
                    bestMatch = student.name;
                }
            }
            return bestMatch;
        }

        // Load face descriptors for known students
        async function loadKnownFaces() {
            const students = [];
            const studentNames = ['student1', 'student2']; // Add more students as needed
            for (let name of studentNames) {
                const img = await faceapi.fetchImage(`students_faces/${name}.jpg`);
                const detections = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor();
                if (detections) {
                    students.push({ name, descriptor: detections.descriptor });
                }
            }
            return students;
        }
    </script>

</body>
</html>
