<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection Attendance</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #f0f0f0;
            flex-direction: column;
        }
        video {
            border: 2px solid black;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
        .student-name {
            font-size: 16px;
            font-weight: bold;
            color: green;
            position: absolute;
            top: 10px;
            left: 10px;
            z-index: 10;
        }
    </style>
</head>
<body>

    <h1>Face Recognition Attendance</h1>
    <video id="video" width="640" height="480" autoplay></video>
    <canvas id="canvas" width="640" height="480"></canvas>

    <script>
        let videoElement = document.getElementById('video');
        let canvasElement = document.getElementById('canvas');
        let studentNameElement = document.createElement('div');
        document.body.appendChild(studentNameElement);

        // Load models from face-api.js
        Promise.all([
            faceapi.nets.ssdMobilenetv1.loadFromUri('/models'),
            faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
            faceapi.nets.faceRecognitionNet.loadFromUri('/models')
        ]).then(startVideo);

        // Set up video stream
        async function startVideo() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            videoElement.srcObject = stream;
        }

        // Detect faces and recognize
        videoElement.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(videoElement);
            document.body.append(canvas);
            const displaySize = { width: videoElement.width, height: videoElement.height };
            faceapi.matchDimensions(canvas, displaySize);

            setInterval(async () => {
                const detections = await faceapi.detectAllFaces(videoElement)
                    .withFaceLandmarks()
                    .withFaceDescriptors();

                // Resize and display detected faces
                canvas?.clear();
                const resizedDetections = faceapi.resizeResults(detections, displaySize);
                canvas?.detectAllFaces(videoElement).draw(resizedDetections);

                // Check each detection for matching faces
                for (let i = 0; i < detections.length; i++) {
                    const descriptor = detections[i].descriptor;
                    let bestMatch = null;

                    // Load pre-saved face descriptors (match against student faces)
                    const students = await getStudentDescriptors();

                    // Compare face descriptor with known students
                    for (let student of students) {
                        const distance = faceapi.euclideanDistance(descriptor, student.descriptor);
                        if (!bestMatch || distance < bestMatch.distance) {
                            bestMatch = { name: student.name, distance: distance };
                        }
                    }

                    if (bestMatch && bestMatch.distance < 0.6) { // Threshold for match
                        studentNameElement.textContent = bestMatch.name;
                        studentNameElement.style.left = `${detections[i].detection.box.x}px`;
                        studentNameElement.style.top = `${detections[i].detection.box.y}px`;
                    }
                }
            }, 100);
        });

        // Load student faces and their descriptors
        async function getStudentDescriptors() {
            const students = [];
            const studentNames = ['student1', 'student2'];  // Example student names
            for (let name of studentNames) {
                const image = await faceapi.fetchImage(`/students_faces/${name}.jpg`);
                const detections = await faceapi.detectSingleFace(image).withFaceLandmarks().withFaceDescriptor();
                if (detections) {
                    students.push({
                        name: name,
                        descriptor: detections.descriptor
                    });
                }
            }
            return students;
        }
    </script>

</body>
</html>
